{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from LightGrad import LightGrad\n",
    "\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_phn_to_id(phonemes, phn2id):\n",
    "    \"\"\"\n",
    "    phonemes: phonemes separated by ' '\n",
    "    phn2id: phn2id dict\n",
    "    \"\"\"\n",
    "    return [phn2id[x] for x in ['<bos>'] + phonemes.split(' ') + ['<eos>']]\n",
    "\n",
    "\n",
    "def text2phnid(text, phn2id, language='zh', add_blank=True):\n",
    "    if language == 'zh':\n",
    "        from text import G2pZh\n",
    "        character2phn = G2pZh()\n",
    "        pinyin, phonemes = character2phn.character2phoneme(text)\n",
    "        if add_blank:\n",
    "            phonemes = ' <blank> '.join(phonemes.split(' '))\n",
    "        return pinyin, phonemes, convert_phn_to_id(phonemes, phn2id)\n",
    "    elif language == 'en':\n",
    "        from text import G2pEn\n",
    "        word2phn = G2pEn()\n",
    "        phonemes = word2phn(text)\n",
    "        if add_blank:\n",
    "            phonemes = ' <blank> '.join(phonemes)\n",
    "        return phonemes, convert_phn_to_id(phonemes, phn2id)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Language should be zh (for Chinese) or en (for English)!')\n",
    "\n",
    "\n",
    "def plot_mel(tensors, titles):\n",
    "    xlim = max([t.shape[1] for t in tensors])\n",
    "    fig, axs = plt.subplots(nrows=len(tensors),\n",
    "                            ncols=1,\n",
    "                            figsize=(12, 9),\n",
    "                            constrained_layout=True)\n",
    "    for i in range(len(tensors)):\n",
    "        im = axs[i].imshow(tensors[i],\n",
    "                           aspect=\"auto\",\n",
    "                           origin=\"lower\",\n",
    "                           interpolation='none')\n",
    "        plt.colorbar(im, ax=axs[i])\n",
    "        axs[i].set_title(titles[i])\n",
    "        axs[i].set_xlim([0, xlim])\n",
    "    fig.canvas.draw()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup HiFi-GAN\n",
    "\n",
    "from hifi_gan import models, env\n",
    "\n",
    "HiFiGAN_CONFIG = ''\n",
    "HiFiGAN_ckpt = ''\n",
    "with open(HiFiGAN_CONFIG) as f:\n",
    "    hifigan_hparams = env.AttrDict(json.load(f))\n",
    "\n",
    "generator = models.Generator(hifigan_hparams)\n",
    "\n",
    "generator.load_state_dict(torch.load(\n",
    "    HiFiGAN_ckpt, map_location='cpu')['generator'])\n",
    "generator = generator.eval()\n",
    "generator.remove_weight_norm()\n",
    "\n",
    "\n",
    "def convert_mel_to_audio(mel):\n",
    "    # only support batch size of 1\n",
    "    assert mel.shape[0] == 1\n",
    "    with torch.no_grad():\n",
    "        audio = generator(mel).squeeze(1)  # (b,t)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference for bznsyp\n",
    "\n",
    "N_STEP = 4\n",
    "TEMP = 1.5\n",
    "STREAMING_CLIP_SIZE = 0.5  # in seconds\n",
    "\n",
    "config_path = 'config/bznsyp_config.yaml'\n",
    "ckpt_path = ''\n",
    "\n",
    "print('loading ', ckpt_path)\n",
    "_, _, state_dict = torch.load(ckpt_path,\n",
    "                              map_location='cpu')\n",
    "\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, yaml.SafeLoader)\n",
    "\n",
    "with open(config['phn2id_path']) as f:\n",
    "    phn2id = json.load(f)\n",
    "vocab_size = len(phn2id) + 1\n",
    "\n",
    "model = LightGrad.build_model(config, vocab_size)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"做一个测试\"\n",
    "\n",
    "pinyin, phonemes, phnid = text2phnid(text, phn2id, 'zh')\n",
    "print(f'pinyin seq: {pinyin}')\n",
    "print(f'phoneme seq: {phonemes}')\n",
    "phnid_len = torch.tensor(len(phnid), dtype=torch.long).unsqueeze(0)\n",
    "phnid = torch.tensor(phnid).unsqueeze(0)\n",
    "\n",
    "mel_clips = []\n",
    "\n",
    "streaming_clip_frames = STREAMING_CLIP_SIZE * config['sample_rate'] // config[\n",
    "    'hop_size']\n",
    "\n",
    "for _, mel_clip, _ in model.forward_streaming(phnid,\n",
    "                                              phnid_len,\n",
    "                                              n_timesteps=N_STEP,\n",
    "                                              temperature=TEMP,\n",
    "                                              out_size=streaming_clip_frames,\n",
    "                                              solver='dpm'):\n",
    "    mel_clips.append(mel_clip)\n",
    "\n",
    "mel_prediction_streaming = torch.cat(mel_clips, dim=2)\n",
    "\n",
    "_, mel_prediction, _ = model.forward(phnid,\n",
    "                                     phnid_len,\n",
    "                                     n_timesteps=N_STEP,\n",
    "                                     temperature=TEMP,\n",
    "                                     solver='dpm')\n",
    "\n",
    "plot_mel([mel_prediction_streaming[0], mel_prediction[0]],\n",
    "         ['streaming inference', 'non-streaming inference'])\n",
    "\n",
    "ipd.display(\n",
    "    ipd.Audio(convert_mel_to_audio(mel_prediction_streaming), rate=22050))\n",
    "ipd.display(ipd.Audio(convert_mel_to_audio(mel_prediction), rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference for ljspeech\n",
    "\n",
    "N_STEP = 4\n",
    "TEMP = 1.5\n",
    "STREAMING_CLIP_SIZE = 0.5  # in seconds\n",
    "\n",
    "config_path = 'config/ljspeech_config.yaml'\n",
    "ckpt_path = ''\n",
    "\n",
    "print('loading ', ckpt_path)\n",
    "_, _, state_dict = torch.load(ckpt_path,\n",
    "                              map_location='cpu')\n",
    "\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, yaml.SafeLoader)\n",
    "\n",
    "with open(config['phn2id_path']) as f:\n",
    "    phn2id = json.load(f)\n",
    "vocab_size = len(phn2id) + 1\n",
    "\n",
    "model = LightGrad.build_model(config, vocab_size)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test\"\n",
    "\n",
    "phonemes, phnid = text2phnid(text, phn2id, 'en')\n",
    "print(f'phoneme seq: {phonemes}', type(phonemes))\n",
    "phnid_len = torch.tensor(len(phnid), dtype=torch.long).unsqueeze(0)\n",
    "phnid = torch.tensor(phnid).unsqueeze(0)\n",
    "\n",
    "mel_clips = []\n",
    "\n",
    "streaming_clip_frames = STREAMING_CLIP_SIZE * config['sample_rate'] // config[\n",
    "    'hop_size']\n",
    "\n",
    "for _, mel_clip, _ in model.forward_streaming(phnid,\n",
    "                                              phnid_len,\n",
    "                                              n_timesteps=N_STEP,\n",
    "                                              temperature=TEMP,\n",
    "                                              out_size=streaming_clip_frames,\n",
    "                                              solver='dpm'):\n",
    "    mel_clips.append(mel_clip)\n",
    "\n",
    "mel_prediction_streaming = torch.cat(mel_clips, dim=2)\n",
    "\n",
    "_, mel_prediction, _ = model.forward(phnid,\n",
    "                                     phnid_len,\n",
    "                                     n_timesteps=N_STEP,\n",
    "                                     temperature=TEMP,\n",
    "                                     solver='dpm')\n",
    "\n",
    "plot_mel([mel_prediction_streaming[0], mel_prediction[0]],\n",
    "         ['streaming inference', 'non-streaming inference'])\n",
    "\n",
    "ipd.display(ipd.Audio(convert_mel_to_audio(\n",
    "    mel_prediction_streaming), rate=22050))\n",
    "ipd.display(ipd.Audio(convert_mel_to_audio(mel_prediction), rate=22050))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "1c27759576147a09f82f75fe7e6da160ee29ac300de0ba196702adc9d307c9a1"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "1059529bf0eac96a858df282bfcfa3b0fdcaa085677d3010c56aeec385ff20b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
